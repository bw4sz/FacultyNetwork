---
title: "Academic Betadiversity and Future of Biology Research"
author: "Ben Weinstein"
date: "Thursday, April 23, 2015"
output: 
html_document: 
  toc: yes
---


```{r,warning=FALSE,message=FALSE}
#Scopus Database Query By Journal Article
library(XML)
library(proto)
library(magrittr)
library(broom)
library(plyr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(httr)
library(stringr)
library(chron)
library(vegan)
library(RSQLite)
library(knitr)
library(bipartite)
library(sna)
library(igraph)
library(knitr)
library(gridExtra)
library(GGally)
library(stringr)
library(networkD3)
```


```{r,}
#source functions
source("Funtions.R")

#set knitr options
opts_chunk$set(echo=T,cache=F,fig.align='center',fig.height=12,fig.width=14,warning=F,message=F)
```

#Read in data. Processed from ByJournal.R

```{r}
#open database
d<-src_sqlite(path = "C:/Users/Ben/Dropbox/FacultyNetwork/Meta.sqlite3")

#what does the data look like?
dbGetQuery(d$con, "SELECT * FROM JA limit 10")
dbGetQuery(d$con, "SELECT * FROM Meta limit 10")

#Order seems to be an issue, lets create a table that just merges the two column and keeps the larger number.

dbGetQuery(d$con, "CREATE TEMP TABLE m AS SELECT * FROM Meta")

dbGetQuery(d$con,"UPDATE m SET Author = `Order` WHERE AUTHOR < 100 ")

#we want to remove authors that have few publications, atleast 4
sq<-"DELETE FROM m WHERE Author in (SELECT Author FROM (SELECT Author, Count(*) as p FROM m GROUP BY Author HAVING p < 4))"
auth<-dbGetQuery(d$con, sq)
```

Basic data cleaning. We only want records from active authors. Atleast 5 publications in the entire record.

```{r}
#i want to do this in sql, but right now i'm just better in R
j_class<-dbGetQuery(d$con,"SELECT * From j_class")

jscopus<-dbGetQuery(d$con,"SELECT * FROM journal_scopus")

JA<-dbGetQuery(d$con,"Select * FROM JA")

# we need metadata for each of the classes, some of the spelling conflicts.
head(jscopus)
head(j_class)
j_class$Source<-toupper(j_class$Publication)
jscopus$Source<-toupper(jscopus$title)

#which need the "the added back"
jscopus$Source[!jscopus$Source %in% j_class$Source]<-paste("THE",jscopus$Source[!jscopus$Source %in% j_class$Source])

jc<-merge(j_class,jscopus)

#send it back to SQL land
copy_to(d,jc,"jc",temporary = T)

dbGetQuery(d$con,"SELECT Author, Year, Discipline, Class, Journal, Count(*) as P From m Inner JOIN JA ON m.DOI=JA.DOI INNER JOIN jc ON Journal = jc.Source  GROUP BY Author, Discipline limit 10")

#in case there are malformed classes
tocompare<-dim(droplevels(tocompare[tocompare$Class %in% j_class$Class,]))

#take year 2015
tocompare<-tocompare[!tocompare$Year %in% 2015,]

dim(tocompare)

#merge with journal - this may need to sql eventually
merge(tocompare,j_class)
```


#Descriptive statistics

```{r}
#How many journals
paste("Number of Journals:",length(unique(tocompare$Journal)))

#How many authors
paste("Number of Authors:",length(unique(tocompare$Author)))

#How many papers
paste("Number of Papers:",length(unique(tocompare$DOI)))

ta<-sort(table(tocompare$Journal))
print("Most published journals")
tail(ta)
```

##How many papers from each discipline over time?

```{r,fig.width=12,fig.height=20}
class_year<-group_by(tocompare,Class,Year) %>% summarize(Papers=length(unique(DOI)))
ggplot(class_year,aes(x=as.factor(Year),y=Papers,col=Class,group=Class)) + geom_line() + theme_bw() + facet_wrap(~Class,scale="free_y",ncol=2) + labs(x="Year")
ggsave("Figures/Papers_Year.svg",dpi=300)
```

Create matrix of authors in each class - analagous to the site by species matrix used in ecology

```{r}
siteXspp<-as.data.frame.array(table(tocompare$Author,tocompare$Class))
dim(siteXspp)
```

#Dissimalarity among classes

Use the abundance of papers by each author to calculate niche overlap (dist=1-Horn's) between classes. 

Low overlap=0
High overlap=1


```{r}
#Compare disciplines
topics<-1-as.matrix(vegdist(t(siteXspp),"horn"))
```

##Visualize interactions

  * Stronger interactions are more red.
  * Weaker interactions are blue.
  * Stronger connections are thicker
  * Weaker connections are thinner
  * More central members (greater number of partners) are in larger text
  * Less central members (fewer number of partners) are in smaller text
  
```{r}
g<-graph.adjacency(topics,"undirected",weighted=TRUE)

g<-simplify(g)

# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)

V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam=E(g)$weight/max(E(g)$weight)
E(g)$color<-rgb(0,1,0,alpha=E(g)$weight/max(E(g)$weight),maxColorValue=1)

ramp <- colorRamp(c("blue","red"),alpha=T)

E(g)$color = apply(ramp(E(g)$weight), 1, function(x) rgb(x[1]/255,x[2]/255,x[3]/255,alpha=T) )


#If you need to delete
g.copy <- delete.edges(g, which(E(g)$weight<.05))
#width
width<-(E(g.copy)$weight/max(E(g.copy)$weight))*8

#label sizes
V(g.copy)$degree <- degree(g.copy)
V(g.copy)$label.cex <- V(g.copy)$degree / max(V(g.copy)$degree)*.5+.5

#get vertex size
size<-group_by(tocompare,Class) %>% summarize(Papers=length(unique(DOI)))
size<-round(as.numeric(table(tocompare$Class))/max(table(tocompare$Class))*5)+2

#plot network
layout1 <- layout.fruchterman.reingold(g.copy,niter=10000,area=vcount(g.copy)^2.3)

plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")
```

```{r,results='hide'}

#save full  network
svg(filename = "Figures/Overall_NetworkIgraph.svg",width=10,height=10)
plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")
dev.off()

jpeg(filename = "Figures/Overall_NetworkIgraph.jpeg",res=300,height=12,width=10,units="in")
plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")
dev.off()

```

##View as a dendrogram

```{r}
wt <- walktrap.community(g, modularity=TRUE)
dend <- as.dendrogram(wt, use.modularity=TRUE)
plot(as.hclust(dend))
```

#Calculate network statistics

We are interested in the centrality, modularity and compartamentalization of the biological sciences

  * Betweenness - number of shortest path that includes node. This tends to find gatekeeps among components.
  * Degree - number of connections for each node
  * Eigenvector centrality: A node is important if it is connected to many important nodes. A node with a small number of influential contacts will be better than a larger number of mediocre contacts.
```{r}
between_class<-betweenness(g.copy)
degree_class<-degree(g.copy)
eigenV<-evcent(g.copy)$vector
vdat<-data.frame(Class=names(between_class),Between=between_class,Degree=degree_class,Eigen=eigenV)
```

##Correlation among measures.

```{r,fig.height=7,fig.width=7}
ggpairs(vdat[,-1])

#reoroder levels
vdat$Class<-factor(vdat$Class,levels=vdat[order(vdat$Between,vdat$Degree),"Class"])
```

##Top actors for each statistic

```{r}

size<-data.frame(Class=colnames(siteXspp),Papers=apply(siteXspp,2,sum))

vdat<-merge(size,vdat)

vdat$Insularity<-vdat$Degree/vdat$Papers
mdat<-melt(vdat)

colnames(mdat)<-c("Class","Metric","Score")

#order and plot
dt<-group_by(mdat,Metric) %>% mutate(Svalue=as.numeric(scale(Score))) %>% group_by(Metric,Class) %>% arrange(Score)

#sort by betweenness
ord<-dcast(dt,Class~Metric) %>% select(Class,Between,Degree) %>% arrange(Between,Degree) %>% select(Class)

dt$Class<-factor(dt$Class,levels=ord$Class)
ggplot(dt[dt$Metric %in% c("Between","Degree","Insularity"),],aes(y=Class,x=Svalue)) + geom_bar_horz(stat="identity",position="identity") + facet_grid(.~Metric,scale="free_x") + labs(x="Z-score",y="Biological Field") + theme_bw()

ggsave("Figures/OveralMetrics.jpg",dpi=300,height=10,width=12)
ggsave("Figures/OveralMetrics.svg",dpi=300)
```

```{r}
d<-dcast(dt,Class~Metric)
#merge with papers
ds<-merge(d,size)

ggplot(ds,aes(x=Between,y=Degree,label=Class,size=log(Papers))) + geom_point(alpha=.3) +  geom_abline() + theme_bw() + scale_size(range=c(10,15)) + geom_text(size=3,hjust=1,vjust=-1) + xlim(-2,2) + ylim(-2,2)

```

##Average connectedness and degree and link strength
```{r}
group_by(mdat,Metric) %>% summarize(mean=mean(Score))
overall.density<-graph.density(g.copy)
mean(E(g.copy)$weight)
```

#Network stats over time
```{r,fig.height=6,fig.width=9}
#split into a time frame 5 years?

m<-seq(1995,2014,2)

tocompare$Time<-cut(as.numeric(as.character(tocompare$Year)),breaks=m,labels=m[1:length(m)-1])

yearcompare<-split(tocompare,tocompare$Time)

#all over 

#caculate degree distribution
dd<-melt(sapply(yearcompare,CalcDD))
ggplot(dd,aes(x=Var1,y=value,col=as.factor(L1))) + geom_line(size=.5) + geom_point(size=4,aes(group=as.factor(L1))) + theme_bw() + xlab("Node Degree") + ggtitle("Degree Distribution") + labs(col="Year") 

yearstats<-lapply(yearcompare,calcN)

yearstats<-melt(yearstats)
ggplot(yearstats,aes(x=L1,y=value,col=Class)) + geom_point() +geom_line(aes(group=Class)) + facet_wrap(~variable,scales="free",ncol=1)

```

##Connectance through time

```{r}
yeardat<-lapply(yearcompare,function(x){
  siteXspp<-droplevels(as.data.frame.array(table(x$Author,x$Class)))
  
  #drop empty rows and colums
  siteXspp<-siteXspp[rownames(siteXspp) %in% names(which(apply(siteXspp,1,sum) > 0)),colnames(siteXspp) %in% names(which(apply(siteXspp,2,sum) > 0))]
  
  #Compare disciplines
  topics<-1-as.matrix(vegdist(t(siteXspp),"horn"))
  diag(topics)<-NA
  topics[upper.tri(topics)]<-NA
  return(topics)
})

yeardat<-melt(yeardat)
colnames(yeardat)<-c("To","From","Niche.Overlap","Year")
```

Remove extremely weak connections 

```{r,fig.width=13}
#remove very weak connections
#work from an example data
#make into characters
yeardat$To<-as.character(yeardat$To)
yeardat$From<-as.character(yeardat$From)

#remove weak connections
exdat<-group_by(yeardat,To,From) %>% filter(Niche.Overlap>0.05 & !is.na(Niche.Overlap))

exdat$Combo<-paste(exdat$To,exdat$From,sep="-")

#plot
ggplot(exdat,aes(x=Year,y=Niche.Overlap,col=From)) + geom_point() + geom_line(aes(group=Combo)) + facet_wrap(~To,scales="free",ncol=3) + theme_bw() + geom_text(data=exdat[exdat$Year==m[length(m)/2],],aes(label=From),size=4) 

ggsave("Figures/LinkTime.svg",dpi=300)
ggsave("Figures/LinkTime.jpeg",height=10,width=14,dpi=300)

```

#Trend estimation

Its not clear to me if i need to use a time-series model. The connection at time A should be independent of connection at time A+1. While they may be related due to the trend - there is no mechanistic connection among publications between years. Its not like a population, where the number of available producers directly influences the offspring in the next year. For the moment, i'm just using a linear model with year as a continious variable. This probably needs to change.


```{r}
exdat$Combo<-paste(exdat$To,exdat$From,sep="-")

#year is a number value for the moment
exdat$Year<-as.numeric(exdat$Year)-1995

sdat<-split(exdat,exdat$Combo)

#get rid of combinations with less than ten years of points
sdat<-sdat[lapply(sdat,nrow) > 2]

#
# Break up d by state, then fit the specified model to each piece and
# return a list
tmod<-rbind_all(lapply(sdat,function(df){
  tdat<-tidy(lm(Niche.Overlap ~ Year, data = df))
  tdat<-tdat[tdat$term =="Year",]
  tdat$Combo<-unique(df$Combo)
  return(tdat)
}))

#extract names into columns
tmod$To<-str_match(tmod$Combo,"(.*)-")[,2]
tmod$From<-str_match(tmod$Combo,"-(.*)")[,2]
```

##Visualize effect over time

Positive values are significantly increasing connections
Negative values are significantly decreasing connections

```{r}
tmod$svalue<-scale(tmod$estimate)

ggplot(tmod[tmod$p.value < 0.05 & tmod$term=="Year",],aes(x=To,y=From,fill=estimate)) + geom_tile() + theme_bw() + scale_fill_gradientn(colours=c("blue","gray","red"),limits=c(-max(tmod$estimate),max(tmod$estimate))) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x="",y="")
ggsave("Figures/InteractionsTime.svg",dpi=300,height=5,width=8)
ggsave("Figures/InteractionsTime.jpeg",height=5,width=8) 

```

#D3 Visualizations

The wonderful D3 package connections!

```{r,cache=FALSE}
MisLinks<-melt(topics)

#remove very weak connections
MisLinks<-MisLinks[MisLinks$value > 0.05,]
MisLinks$value<-MisLinks$value*10
colnames(MisLinks)<-c("To","From","value")
MisLinks$To<-as.character(MisLinks$To)
MisLinks$From<-as.character(MisLinks$From)

MisNodes<-data.frame(name=as.factor(sort(as.character(unique(c(MisLinks$To,MisLinks$From))))))
#Add groups
MisNodes$group<-as.integer(1)

MisLinks$source<-as.integer(sapply(MisLinks$To,function(x) which(x ==MisNodes$name)))-1
MisLinks$target<-as.integer(sapply(MisLinks$From,function(x) which(x ==MisNodes$name)))-1

#Order by source
MisLinks<-MisLinks[order(MisLinks$source),]

simpleNetwork(MisLinks,fontSize = 15)
```

```{r,cache=F}
simpleNetwork(MisLinks,height=500,width=700)  %>% saveNetwork(file = 'Figures/Net1.html',selfcontained=F)
```

##Force based network

```{r,cache=F}
d3Network::d3ForceNetwork(Links = MisLinks, Nodes = MisNodes, Source = "source",Target = "target", Value = "value", NodeID = "name", Group = "group", opacity = 0.9,file="Figures/Net3.html") 

```


```{r}
save.image("Analysis.RData")
```
