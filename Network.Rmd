---
title: "Academic Betadiversity and Future of Biology Research"
author: "Ben Weinstein"
date: "Thursday, April 23, 2015"
output: html_document
---


```{r,warning=FALSE,message=FALSE}
#Scopus Database Query By Journal Article
library(XML)
library(proto)
library(magrittr)
library(broom)
library(plyr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(httr)
library(stringr)
library(chron)
library(vegan)
library(knitr)
library(bipartite)
library(sna)
library(igraph)
library(knitr)
library(gridExtra)
library(GGally)
library(stringr)
library(networkD3)
```


```{r,}
#source functions
source("Funtions.R")

#set knitr options
opts_chunk$set(echo=T,cache=F,fig.align='center',fig.height=15,fig.width=14,warning=F,message=F)
```

Read in data. Processed from ByJournal.R
```{r}
#journal class
journaldf<-read.csv("C:/Users/Ben/Dropbox/FacultyNetwork/JournalID.csv",row.names=1)

tocompare<-read.table("C:/Users/Ben/Dropbox/FacultyNetwork/ParsedDataID.csv",row.names=NULL,header=T,sep=",",fill=T)

#what does it look like:
tail(tocompare)

dim(tocompare)

#remove biogeneral i think for now
tocompare<-tocompare[!tocompare$Class %in% "Biogeneral",]
```

Basic data cleaning. We only want records from active authors. Atleast 5 publications in the entire record.

```{r}
#filter authors
#distribution of publication
keep<-names(which(table(tocompare$Author)>3))
tocompare<-droplevels(tocompare[tocompare$Author %in% keep,])

#remove duplicates.
tocompare<-tocompare[!duplicated(tocompare),]

#in case the column names in there 
tocompare<-droplevels(tocompare[!tocompare$Journal %in% "DOI",])
dim(tocompare)

j_class<-read.csv("Class.csv",row.names=1)

#in case there are malformed classes
tocompare<-droplevels(tocompare[tocompare$Class %in% j_class$Class,])

#take out malformed lines
tocompare<-tocompare[which(!str_detect(tocompare$Journal, "SCOPUS")),]

#take year 2015
tocompare<-tocompare[!tocompare$Year %in% 2015,]

tocompare<-droplevels(tocompare[which(!str_detect(tocompare$h5.index, "SCOPUS")),])
dim(tocompare)

```


Basic descriptive stats on results
```{r}
#How many journals
paste("Number of Journals:",length(unique(tocompare$Journal)))

#How many authors
paste("Number of Authors:",length(unique(tocompare$Author)))

#How many papers
paste("Number of Papers:",length(unique(tocompare$DOI)))

ta<-sort(table(tocompare$Journal))
print("Most published journals")
tail(ta)
```

How many papers from each discipline over time?
```{r}
class_year<-group_by(tocompare,Class,Year) %>% summarize(Papers=length(unique(DOI)))
ggplot(class_year,aes(x=as.factor(Year),y=Papers,col=Class,group=Class)) + geom_line() + theme_bw() + facet_wrap(~Class,scale="free_y",ncol=4) + labs(x="Year")
ggsave("Figures/Papers_Year.svg",dpi=300)
```

Create matrix of authors in each class - analagous to the site by species matrix used in ecology

```{r}
siteXspp<-as.data.frame.array(table(tocompare$Author,tocompare$Class))
dim(siteXspp)
```

#Dissimalarity among classes

Use the abundance of papers by each author to calculate niche overlap (dist=1-Horn's) between classes. 

Low overlap=0
High overlap=1


```{r}
#Compare disciplines
topics<-1-as.matrix(vegdist(t(siteXspp),"horn"))
```

Visualize interactions.

  * Stronger interactions are more red.
  * Weaker interactions are blue.
  * Stronger connections are thicker
  * Weaker connections are thinner
  * More central members (greater number of partners) are in larger text
  * Less central members (fewer number of partners) are in smaller text
  
```{r}
g<-graph.adjacency(topics,"undirected",weighted=TRUE)

g<-simplify(g)

# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)

V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam=E(g)$weight/max(E(g)$weight)
E(g)$color<-rgb(0,1,0,alpha=E(g)$weight/max(E(g)$weight),maxColorValue=1)

ramp <- colorRamp(c("blue","red"),alpha=T)

E(g)$color = apply(ramp(E(g)$weight), 1, function(x) rgb(x[1]/255,x[2]/255,x[3]/255,alpha=T) )


#If you need to delete
g.copy <- delete.edges(g, which(E(g)$weight<.05))
#width
width<-(E(g.copy)$weight/max(E(g.copy)$weight))*8

#label sizes
V(g.copy)$degree <- degree(g.copy)
V(g.copy)$label.cex <- V(g.copy)$degree / max(V(g.copy)$degree)*.5+.5

#get vertex size
size<-group_by(tocompare,Class) %>% summarize(Papers=length(unique(DOI)))
size<-round(as.numeric(table(tocompare$Class))/max(table(tocompare$Class))*5)+2

#scale
plot.igraph(g.copy,vertex.size=size,edge.width=width)

#try a couple different layouts
# plot the graph in layout1
layout1 <- layout.fruchterman.reingold(g.copy,niter=10000,area=vcount(g.copy)^2.3)

plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")

# plot the graph in layout1
layout2 <- layout.reingold.tilford(g.copy,circular=T)

plot(g.copy,edge.width=width,vertex.size=size,layout=layout2,vertex.color="grey70")

# plot the graph in layout3
layout3 <- layout.kamada.kawai(g.copy,niter=5000)

plot(g.copy,edge.width=width,vertex.size=size,layout=layout3,vertex.color="grey40")



#save full  network
svg(filename = "Figures/Overall_NetworkIgraph.svg",width=10,height=10)
plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")
dev.off()

jpeg(filename = "Figures/Overall_NetworkIgraph.jpeg",res=300,height=12,width=10,units="in")
plot(g.copy,edge.width=width,vertex.size=size,layout=layout1,vertex.color="grey40")
dev.off()

```

View as a dendrogram

```{r}
wt <- walktrap.community(g, modularity=TRUE)
dend <- as.dendrogram(wt, use.modularity=TRUE)
plot(as.hclust(dend))
```

#Calculate network statistics

We are interested in the centrality, modularity and compartamentalization of the biological sciences

  * Betweenness - number of shortest path that includes node. This tends to find gatekeeps among components.
  * Degree - number of connections for each node
  * Closeness - relative distance to all actors
  * Coreness - The centrality of network position.
  * Eigenvector centrality: A node is important if it is connected to many important nodes. A node with a small number of influential contacts will be better than a larger number of mediocre contacts.
```{r}
between_class<-betweenness(g.copy)
degree_class<-degree(g.copy)
closeness_class<-closeness(g.copy)
eigenV<-evcent(g.copy)$vector
vdat<-data.frame(Class=names(between_class),Between=between_class,Degree=degree_class,Closeness=closeness_class,Eigen=eigenV)
```

Correlation among importance measures.

```{r}
ggpairs(vdat[,-1])

#reoroder levels
vdat$Class<-factor(vdat$Class,levels=vdat[order(vdat$Between,vdat$Degree),"Class"])
```

Top actors for each statistic
```{r}
mdat<-melt(vdat)
colnames(mdat)<-c("Class","Metric","Score")
#order and plot
dt<-group_by(mdat,Metric) %>% mutate(Svalue=as.numeric(scale(Score))) %>% group_by(Metric,Class) %>% arrange(Score)

ggplot(dt,aes(y=Class,x=Svalue)) + geom_bar_horz(stat="identity",position="identity") + facet_grid(.~Metric) + labs(x="Z-score")
ggsave("Figures/OveralMetrics.jpg",dpi=300,height=10,width=12)
ggsave("Figures/OveralMetrics.svg",dpi=300)
```

##Average connectedness and degree and link strength
```{r}
group_by(mdat,Metric) %>% summarize(mean=mean(Score))
overall.density<-graph.density(g.copy)
mean(E(g.copy)$weight)
```

##Network stats over time
```{r}
#split into a time frame 5 years?

m<-seq(1995,2014,2)

tocompare$Time<-cut(as.numeric(as.character(tocompare$Year)),breaks=m,labels=m[1:length(m)-1])

yearcompare<-split(tocompare,tocompare$Time)

#all over 

#caculate degree distribution
dd<-melt(sapply(yearcompare,CalcDD))
ggplot(dd,aes(x=Var1,y=value,col=as.factor(L1))) + geom_line(size=.5) + geom_point(size=4,aes(group=as.factor(L1))) + theme_bw() + xlab("Node Degree") + ggtitle("Degree Distribution") + labs(col="Year") 

yearstats<-lapply(yearcompare,calcN)

head(yearstats)
yearstats<-melt(yearstats)
ggplot(yearstats,aes(x=L1,y=value,col=Class)) + geom_point() +geom_line(aes(group=Class)) + facet_wrap(~variable,scales="free",ncol=1)


```

##Connectance through time

```{r}
yeardat<-lapply(yearcompare,function(x){
  siteXspp<-droplevels(as.data.frame.array(table(x$Author,x$Class)))
  
  #drop empty rows and colums
  siteXspp<-siteXspp[rownames(siteXspp) %in% names(which(apply(siteXspp,1,sum) > 0)),colnames(siteXspp) %in% names(which(apply(siteXspp,2,sum) > 0))]
  
  #Compare disciplines
  topics<-1-as.matrix(vegdist(t(siteXspp),"horn"))
  diag(topics)<-NA
  topics[upper.tri(topics)]<-NA
  return(topics)
})

yeardat<-melt(yeardat)
colnames(yeardat)<-c("To","From","Niche.Overlap","Year")
```

Remove extremely weak connections 
```{r,fig.width=13}
#remove very weak connections
#work from an example data
#make into characters
yeardat$To<-as.character(yeardat$To)
yeardat$From<-as.character(yeardat$From)

#remove weak connections
exdat<-group_by(yeardat,To,From) %>% filter(Niche.Overlap>0.05 & !is.na(Niche.Overlap))

exdat$Combo<-paste(exdat$To,exdat$From,sep="-")

#plot
ggplot(exdat,aes(x=Year,y=Niche.Overlap,col=From)) + geom_point() + geom_line(aes(group=Combo)) + facet_wrap(~To,scales="free",ncol=3) + theme_bw() + geom_text(data=exdat[exdat$Year==m[length(m)/2],],aes(label=From),size=4) 

ggsave("Figures/LinkTime.svg",dpi=300)
ggsave("Figures/LinkTime.jpeg",height=10,width=14,dpi=300)

```

##trend estimation

Its not clear to me if i need to use a time-series model. The connection at time A should be independent of connection at time A+1. While they may be related due to the trend - there is no mechanistic connection among publications between years. Its not like a population, where the number of available producers directly influences the offspring in the next year. For the moment, i'm just using a linear model with year as a continious variable. This probably needs to change.

first pass just fit a linear line and determine if its positive or negative

```{r}
exdat$Combo<-paste(exdat$To,exdat$From,sep="-")

#year is a number value for the moment
exdat$Year<-as.numeric(exdat$Year)-1995

sdat<-split(exdat,exdat$Combo)

#get rid of combinations with less than ten years of points
sdat<-sdat[lapply(sdat,nrow) > 2]

#
# Break up d by state, then fit the specified model to each piece and
# return a list
tmod<-rbind_all(lapply(sdat,function(df){
  tdat<-tidy(lm(Niche.Overlap ~ Year, data = df))
  tdat<-tdat[tdat$term =="Year",]
  tdat$Combo<-unique(df$Combo)
  return(tdat)
}))

#extract names into columns
tmod$To<-str_match(tmod$Combo,"(.*)-")[,2]
tmod$From<-str_match(tmod$Combo,"-(.*)")[,2]
```

Visualize effect over time

Positive values are significantly increasing connections
Negative values are significantly decreasing connections

```{r}
tmod$svalue<-scale(tmod$estimate)

ggplot(tmod[tmod$p.value < 0.05 & tmod$term=="Year",],aes(x=To,y=From,fill=estimate)) + geom_tile() + theme_bw() + scale_fill_gradientn(colours=c("blue","gray","red"),limits=c(-max(tmod$estimate),max(tmod$estimate))) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x="",y="")
ggsave("InteractionsTime.svg",dpi=300)
ggsave("InteractionsTime.jpeg",height=7,width=7) 

```

#D3 Networks

###D3 Visualizations

The wonderful D3 package connections!

```{r,cache=FALSE}
MisLinks<-melt(topics)

#remove very weak connections
MisLinks<-MisLinks[MisLinks$value > 0.05,]
MisLinks$value<-MisLinks$value*10
colnames(MisLinks)<-c("To","From","value")
MisLinks$To<-as.character(MisLinks$To)
MisLinks$From<-as.character(MisLinks$From)

MisNodes<-data.frame(name=as.factor(sort(as.character(unique(c(MisLinks$To,MisLinks$From))))))
#Add groups
MisNodes$group<-as.integer(1)

MisLinks$source<-as.integer(sapply(MisLinks$To,function(x) which(x ==MisNodes$name)))-1
MisLinks$target<-as.integer(sapply(MisLinks$From,function(x) which(x ==MisNodes$name)))-1

#Order by source
MisLinks<-MisLinks[order(MisLinks$source),]

simpleNetwork(MisLinks,fontSize = 15)
```

```{r,cache=F}
simpleNetwork(MisLinks,height=500,width=700)  %>% saveNetwork(file = 'Net1.html',selfcontained=F)
```

##Force based network

```{r,cache=F}
d3Network::d3ForceNetwork(Links = MisLinks, Nodes = MisNodes, Source = "source",Target = "target", Value = "value", NodeID = "name", Group = "group", opacity = 0.9,file="Net3.html") 

```


```{r}
save.image("Analysis.RData")
```
